{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# Load the dataset\n",
        "(dataset_train, labels_train), (dataset_test, labels_test) =  fashion_mnist.load_data()  \n",
        "\n",
        "# Preprocess the data\n",
        "dataset_train = dataset_train.astype('float32') / 255\n",
        "dataset_test = dataset_test.astype('float32') / 255\n",
        "\n",
        "if len(dataset_train.shape) == 3:\n",
        "    dataset_train = np.expand_dims(dataset_train, axis=-1)\n",
        "    dataset_test = np.expand_dims(dataset_test, axis=-1)\n",
        "\n",
        "# One-hot encode the labels\n",
        "labels_train = to_categorical(labels_train, 10)\n",
        "labels_test = to_categorical(labels_test, 10)\n"
      ],
      "metadata": {
        "id": "0KprG2LoF_Q0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-addons\n",
        "!pip install -U tensorflow-hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG_mAVBKH2Cs",
        "outputId": "fbabeb62-5e70-46ee-faa8-f60e463c6c79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    dataset_rgb = np.repeat(dataset, 3, axis=-1)  # Convert to RGB\n",
        "    return dataset_rgb\n",
        "\n",
        "# Preprocess the dataset without resizing\n",
        "preprocessed_train = preprocess_dataset(dataset_train)\n",
        "preprocessed_test = preprocess_dataset(dataset_test)\n",
        "\n",
        "# Functions to create simple neural network and LeNet-5 models\n",
        "def simple_nn(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Flatten(input_shape=input_shape),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def lenet_5(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(6, (5, 5), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(16, (5, 5), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(120, activation='relu'),\n",
        "        layers.Dense(84, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate simple neural network and LeNet-5 models\n",
        "input_shape = preprocessed_train.shape[1:]\n",
        "num_classes = 10\n",
        "\n",
        "for model_architecture in [simple_nn, lenet_5]:\n",
        "    model = model_architecture(input_shape, num_classes)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(preprocessed_train, labels_train, epochs=1, batch_size=1, validation_split=0.2)\n",
        "    model_score = model.evaluate(preprocessed_test, labels_test)\n",
        "    print(f\"{model_architecture.__name__} Test Loss:\", model_score[0])\n",
        "    print(f\"{model_architecture.__name__} Test Accuracy:\", model_score[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqHUkgyTNu5Z",
        "outputId": "c3a4efc4-5569-44df-d2b4-2bd9e13ec8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 3423/48000 [=>............................] - ETA: 5:29 - loss: 0.8700 - accuracy: 0.6784"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sHqddt9GGXjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "3D3lqc57H4tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the pretrained models and apply transfer learning:"
      ],
      "metadata": {
        "id": "RYpVNtd4H_H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_dataset(dataset, target_size):\n",
        "    dataset_rgb = np.repeat(dataset, 3, axis=-1)  # Convert to RGB\n",
        "    resized_dataset = np.array([cv2.resize(img, target_size) for img in dataset_rgb])  # Resize the images\n",
        "    return resized_dataset\n",
        "\n",
        "# Preprocess the dataset\n",
        "target_size = (224, 224)  # Use (224, 224) for EfficientNet and (160, 160) for BiT\n",
        "preprocessed_train = preprocess_dataset(dataset_train, target_size)\n",
        "preprocessed_test = preprocess_dataset(dataset_test, target_size)\n",
        "\n",
        "# Function to create a model with transfer learning\n",
        "def create_tl_model(model_url, input_shape, num_classes):\n",
        "    base_model = hub.KerasLayer(model_url, trainable=True)\n",
        "    model = models.Sequential([\n",
        "        layers.InputLayer(input_shape),\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# EfficientNet\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
        "efficientnet_model = create_tl_model(efficientnet_url, target_size+(3,), num_classes)\n",
        "efficientnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "efficientnet_model.fit(preprocessed_train, labels_train, epochs=1, batch_size=1, validation_split=0.2)\n",
        "\n",
        "\n",
        "# BiT\n",
        "bit_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
        "bit_model = create_tl_model(bit_url, target_size+(3,), num_classes)\n",
        "bit_model.compile(optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "bit_model.fit(preprocessed_train, labels_train, epochs=1, batch_size=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "cw5atwR-IAOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Image augmentation:"
      ],
      "metadata": {
        "id": "MgQzPPJQIDlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Train the model with augmented data\n",
        "augmented_model = simple_nn(input_shape, num_classes)  # You can replace this with another model like lenet_5 or the transfer learning models\n",
        "augmented_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "augmented_model.fit(datagen.flow(dataset_train, labels_train, batch_size=1), epochs=1, validation_data=(dataset_test, labels_test))\n"
      ],
      "metadata": {
        "id": "3GPF8eLwIEOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) State-of-the-art image models - MLP-Mixer:\n",
        "\n"
      ],
      "metadata": {
        "id": "ViNbd5izI6Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U timm\n",
        "import timm\n",
        "# Create MLP-Mixer model\n",
        "def create_mlp_mixer(input_shape, num_classes):\n",
        "    model = timm.create_model(\"mixer_b16_224\", pretrained=True, num_classes=num_classes, in_chans=1)\n",
        "    return model\n",
        "\n",
        "# Train MLP-Mixer\n",
        "mlp_mixer_model = create_mlp_mixer(input_shape, num_classes)\n",
        "mlp_mixer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "mlp_mixer_model.fit(dataset_train, labels_train, epochs=1, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "if66zKukI62w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}