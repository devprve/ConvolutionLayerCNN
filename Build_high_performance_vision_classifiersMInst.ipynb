{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load and preprocess the datasets (MNIST, Fashion MNIST, and CIFAR-10):\n",
        "python\n"
      ],
      "metadata": {
        "id": "GKHU13FWS8Kn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0YRw-gqQNqZ",
        "outputId": "9f4c00b8-a113-407e-a3f6-3a03f60d86e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def load_and_preprocess_data(dataset_name):\n",
        "    if dataset_name == 'mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    elif dataset_name == 'fashion_mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    elif dataset_name == 'cifar10':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid dataset name. Choose from 'mnist', 'fashion_mnist', or 'cifar10'.\")\n",
        "\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "    y_train = to_categorical(y_train, 10)\n",
        "    y_test = to_categorical(y_test, 10)\n",
        "\n",
        "    if len(x_train.shape) < 4:  # If the dataset is grayscale, convert to RGB\n",
        "        x_train = np.repeat(x_train[..., np.newaxis], 3, axis=-1)\n",
        "        x_test = np.repeat(x_test[..., np.newaxis], 3, axis=-1)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "dataset_name = \"mnist\"  # Change this to 'fashion_mnist' or 'cifar10' for other datasets\n",
        "x_train, y_train, x_test, y_test = load_and_preprocess_data(dataset_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Use simple neural network as well as LeNet-5 - two architectures:"
      ],
      "metadata": {
        "id": "iYz1OvfuS_oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_nn(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Flatten(input_shape=input_shape),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def lenet_5(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(6, (5, 5), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(16, (5, 5), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(120, activation='relu'),\n",
        "        layers.Dense(84, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_shape = x_train.shape[1:]\n",
        "num_classes = 10\n",
        "\n",
        "for model_architecture in [simple_nn, lenet_5]:\n",
        "    model = model_architecture(input_shape, num_classes)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
        "    model_score = model.evaluate(x_test, y_test)\n",
        "    print(f\"{model_architecture.__name__} Test Loss:\", model_score[0])\n",
        "    print(f\"{model_architecture.__name__} Test Accuracy:\", model_score[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83P-juekQQqy",
        "outputId": "bb4daf29-3909-4f6b-a5a9-919f3795b824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 18s 43ms/step - loss: 0.3057 - accuracy: 0.9124 - val_loss: 0.1725 - val_accuracy: 0.9518\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.1441 - accuracy: 0.9583 - val_loss: 0.1264 - val_accuracy: 0.9640\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.1013 - accuracy: 0.9702 - val_loss: 0.1085 - val_accuracy: 0.9676\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.0762 - accuracy: 0.9781 - val_loss: 0.0991 - val_accuracy: 0.9705\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.0586 - accuracy: 0.9827 - val_loss: 0.0911 - val_accuracy: 0.9735\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.0468 - accuracy: 0.9861 - val_loss: 0.0885 - val_accuracy: 0.9737\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.0366 - accuracy: 0.9896 - val_loss: 0.0932 - val_accuracy: 0.9727\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.0311 - accuracy: 0.9910 - val_loss: 0.0889 - val_accuracy: 0.9738\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.0239 - accuracy: 0.9934 - val_loss: 0.0905 - val_accuracy: 0.9763\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0200 - accuracy: 0.9947 - val_loss: 0.0912 - val_accuracy: 0.9758\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0766 - accuracy: 0.9764\n",
            "simple_nn Test Loss: 0.07662998884916306\n",
            "simple_nn Test Accuracy: 0.9764000177383423\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 36s 91ms/step - loss: 0.3372 - accuracy: 0.8990 - val_loss: 0.1136 - val_accuracy: 0.9672\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 32s 84ms/step - loss: 0.0973 - accuracy: 0.9692 - val_loss: 0.1006 - val_accuracy: 0.9712\n",
            "Epoch 3/10\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9780"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n"
      ],
      "metadata": {
        "id": "uIoM0o3KSAg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Demonstrate pretrained model with transfer learning (both EfficientNet and BiT):\n",
        "\n"
      ],
      "metadata": {
        "id": "uUYLKgC7TB_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install -U tensorflow\n",
        "!pip install tensorflow_hub\n",
        "\n",
        "# 2. Import libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 3. Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data by converting the grayscale images to 3-channel images\n",
        "def preprocess_data(images, labels):\n",
        "    images = np.expand_dims(images, axis=-1)\n",
        "    images = images.astype(\"float32\") / 255.0\n",
        "    images = np.repeat(images, 3, axis=-1)  # Convert to 3-channel images\n",
        "    images = tf.image.resize(images, (32, 32))\n",
        "    labels = tf.keras.utils.to_categorical(labels, 10)\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train, y_train = preprocess_data(x_train, y_train)\n",
        "x_test, y_test = preprocess_data(x_test, y_test)\n",
        "\n",
        "# 4. Load pre-trained models (EfficientNet and BiT)\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
        "bit_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
        "\n",
        "def create_model(model_url, input_shape, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
        "        hub.KerasLayer(model_url, trainable=True),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "efficientnet_model = create_model(efficientnet_url, (32, 32, 3), 10)\n",
        "bit_model = create_model(bit_url, (32, 32, 3), 10)\n",
        "\n",
        "\n",
        "# 5. Fine-tune the models\n",
        "efficientnet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                           loss='categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "bit_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "print(\"Training EfficientNet...\")\n",
        "efficientnet_history = efficientnet_model.fit(x_train, y_train, batch_size=64, epochs=1, validation_split=0.1)\n",
        "\n",
        "print(\"\\nTraining BiT...\")\n",
        "bit_history = bit_model.fit(x_train, y_train, batch_size=64, epochs=1, validation_split=0.1)\n",
        "\n",
        "# 6. Evaluate the models\n",
        "efficientnet_score = efficientnet_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"\\nEfficientNet Test loss:\", efficientnet_score[0])\n",
        "print(\"EfficientNet Test accuracy:\", efficientnet_score[1])\n",
        "\n",
        "bit_score = bit_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"\\nBiT Test loss:\", bit_score[0])\n",
        "print(\"BiT Test accuracy:\", bit_score[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au2ok1AmQj82",
        "outputId": "e64ddfba-e213-4cc4-9127-469f488dc1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.25)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.23.5)\n",
            "Training EfficientNet...\n",
            "844/844 [==============================] - 648s 721ms/step - loss: 0.3150 - accuracy: 0.9477 - val_loss: 0.1963 - val_accuracy: 0.9835\n",
            "\n",
            "Training BiT...\n",
            "844/844 [==============================] - 4719s 6s/step - loss: 0.5021 - accuracy: 0.8803 - val_loss: 0.0634 - val_accuracy: 0.9838\n",
            "\n",
            "EfficientNet Test loss: 0.194539874792099\n",
            "EfficientNet Test accuracy: 0.9825000166893005\n",
            "\n",
            "BiT Test loss: 0.05297866836190224\n",
            "BiT Test accuracy: 0.9861999750137329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Demonstrate image augmentation:"
      ],
      "metadata": {
        "id": "LzRZqxn3TJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "    layers.experimental.preprocessing.Rescaling(1./255),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "datagen.fit(x_train.reshape(-1, 28, 28, 1))\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model.fit(datagen.flow(x_train.reshape(-1, 28, 28, 1), y_train, batch_size=32), epochs=1, validation_data=(x_test.reshape(-1, 28, 28, 1), y_test))\n"
      ],
      "metadata": {
        "id": "khmvt_BqQmI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate some augmented images\n",
        "augmented_images = datagen.flow(x_train.reshape(-1, 28, 28, 1), y_train, batch_size=1)\n",
        "\n",
        "# Display the first 9 augmented images\n",
        "fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
        "axs = axs.ravel()\n",
        "for i in range(9):\n",
        "    image, label = augmented_images.next()\n",
        "    axs[i].imshow(image[0], cmap=plt.cm.gray)\n",
        "    axs[i].set_title(f\"Label: {label.argmax()}\")\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pP-NfTiPFsSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) demonstrate with state of art image models - mlp-mixer"
      ],
      "metadata": {
        "id": "R35-V_p4Dlnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n"
      ],
      "metadata": {
        "id": "OAoy70_QLwFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "JmSj3PP2LL4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install tensorflow-addons\n",
        "!pip install -U tensorflow\n",
        "\n",
        "# 2. Import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# 3. Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# 4. Implement the MLP-Mixer model\n",
        "def mlp_block(units, dropout_rate=0.1):\n",
        "    return keras.Sequential([\n",
        "        layers.Dense(units, activation=tf.nn.gelu),\n",
        "        layers.Dropout(dropout_rate),\n",
        "    ])\n",
        "\n",
        "def mixer_block(hidden_dim, num_patches, dropout_rate=0.1):\n",
        "    return keras.Sequential([\n",
        "        layers.LayerNormalization(),\n",
        "        layers.Dense(num_patches, use_bias=False),\n",
        "        layers.Activation(tf.nn.gelu),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.LayerNormalization(),\n",
        "        layers.Dense(hidden_dim, use_bias=False),\n",
        "        layers.Dropout(dropout_rate),\n",
        "    ])\n",
        "\n",
        "def mlp_mixer(input_shape, hidden_dim, num_blocks, num_patches, dropout_rate=0.1):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Reshape((-1, input_shape[-1]))(inputs)\n",
        "    \n",
        "    for _ in range(num_blocks):\n",
        "        x = layers.Add()([x, mlp_block(hidden_dim)(mixer_block(hidden_dim, num_patches)(x))])\n",
        "\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# 5. Train the model\n",
        "input_shape = (28, 28, 1)\n",
        "hidden_dim = 64\n",
        "num_blocks = 8\n",
        "num_patches = 16\n",
        "\n",
        "model = mlp_mixer(input_shape, hidden_dim, num_blocks, num_patches)\n",
        "model.compile(optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=256, epochs=2, validation_split=0.1)\n",
        "\n",
        "# 6. Evaluate the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n"
      ],
      "metadata": {
        "id": "xt7eIRawLCDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}